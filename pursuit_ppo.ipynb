{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/southrussian/pettingzoo_pursuit_ppo/blob/main/pursuit_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMtaUv7wi5GR"
      },
      "outputs": [],
      "source": [
        "!pip install supersuit --quiet\n",
        "!pip install pettingzoo --quiet\n",
        "!pip install pymunk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.\n",
        "\n",
        "This code is exceedingly basic, with no logging or weights saving.\n",
        "The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.\n",
        "\n",
        "Author: Jet (https://github.com/jjshoots)\n",
        "\"\"\"\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "# from pettingzoo.butterfly import pistonball_v6, cooperative_pong_v5\n",
        "from pettingzoo.mpe import simple_adversary_v3\n",
        "from pettingzoo.sisl import pursuit_v4\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            self._layer_init(nn.Linear(128 * 8 * 8, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
        "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
        "\n",
        "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
        "        torch.nn.init.orthogonal_(layer.weight, std)\n",
        "        torch.nn.init.constant_(layer.bias, bias_const)\n",
        "        return layer\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(self.network(x / 255.0))\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        hidden = self.network(x / 255.0)\n",
        "        logits = self.actor(hidden)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
        "\n",
        "\n",
        "def batchify_obs(obs, device):\n",
        "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
        "    # convert to list of np arrays\n",
        "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
        "    # transpose to be (batch, channel, height, width)\n",
        "    obs = obs.transpose(0, -1, 1, 2)\n",
        "    # convert to torch\n",
        "    obs = torch.tensor(obs).to(device)\n",
        "\n",
        "    return obs\n",
        "\n",
        "\n",
        "def batchify(x, device):\n",
        "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
        "    # convert to list of np arrays\n",
        "    x = np.stack([x[a] for a in x], axis=0)\n",
        "    # convert to torch\n",
        "    x = torch.tensor(x).to(device)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def unbatchify(x, env):\n",
        "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
        "    x = x.cpu().numpy()\n",
        "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    writer = SummaryWriter(f\"runs/0\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\"\n",
        "        % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    \"\"\"ALGO PARAMS\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ent_coef = 0.1\n",
        "    vf_coef = 0.1\n",
        "    clip_coef = 0.1\n",
        "    gamma = 0.99\n",
        "    batch_size = 32\n",
        "    stack_size = 4\n",
        "    frame_size = (64, 64)\n",
        "    max_cycles = 125\n",
        "    total_episodes = 2\n",
        "\n",
        "    \"\"\" ENV SETUP \"\"\"\n",
        "    # env = cooperative_pong_v5.parallel_env(render_mode=\"rgb_array\",\n",
        "    #                                        ball_speed=9, left_paddle_speed=12,\n",
        "    #                                        right_paddle_speed=12, cake_paddle=True,\n",
        "    #                                        max_cycles=900, bounce_randomness=False,\n",
        "    #                                        max_reward=100, off_screen_penalty=-10)\n",
        "    env = pursuit_v4.parallel_env(render_mode=\"human\", max_cycles=max_cycles)\n",
        "    # env = pistonball_v6.parallel_env(\n",
        "    #     render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles\n",
        "    # )\n",
        "    env = color_reduction_v0(env)\n",
        "    env = resize_v1(env, frame_size[0], frame_size[1])\n",
        "    env = frame_stack_v1(env, stack_size=stack_size)\n",
        "    num_agents = len(env.possible_agents)\n",
        "    num_actions = env.action_space(env.possible_agents[0]).n\n",
        "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
        "\n",
        "    \"\"\" LEARNER SETUP \"\"\"\n",
        "    agent = Agent(num_actions=num_actions).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
        "\n",
        "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
        "    end_step = 0\n",
        "    total_episodic_return = 0\n",
        "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
        "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "\n",
        "    \"\"\" TRAINING LOGIC \"\"\"\n",
        "    # train for n number of episodes\n",
        "    for episode in range(total_episodes):\n",
        "        # collect an episode\n",
        "        with torch.no_grad():\n",
        "            global_step = 0\n",
        "            start_time = time.time()\n",
        "            # collect observations and convert to batch of torch tensors\n",
        "            next_obs, info = env.reset()\n",
        "            # reset the episodic return\n",
        "            total_episodic_return = 0\n",
        "\n",
        "            # each episode has num_steps\n",
        "            for step in range(0, max_cycles):\n",
        "                global_step += 1\n",
        "                # rollover the observation\n",
        "                obs = batchify_obs(next_obs, device)\n",
        "\n",
        "                # get action from the agent\n",
        "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
        "\n",
        "                # execute the environment and log data\n",
        "                next_obs, rewards, terms, truncs, infos = env.step(\n",
        "                    unbatchify(actions, env)\n",
        "                )\n",
        "\n",
        "                for idx, item in enumerate(info):\n",
        "                  player_idx = idx % 2\n",
        "                  if \"episode\" in item.keys():\n",
        "                      print(\n",
        "                          f\"global_step={global_step}, {player_idx}-episodic_return={item['episode']['r']}\"\n",
        "                      )\n",
        "                      writer.add_scalar(\n",
        "                          f\"charts/episodic_return-player{player_idx}\",\n",
        "                          item[\"episode\"][\"r\"],\n",
        "                          global_step,\n",
        "                      )\n",
        "                      writer.add_scalar(\n",
        "                          f\"charts/episodic_length-player{player_idx}\",\n",
        "                          item[\"episode\"][\"l\"],\n",
        "                          global_step,\n",
        "                      )\n",
        "\n",
        "                # add to episode storage\n",
        "                rb_obs[step] = obs\n",
        "                rb_rewards[step] = batchify(rewards, device)\n",
        "                rb_terms[step] = batchify(terms, device)\n",
        "                rb_actions[step] = actions\n",
        "                rb_logprobs[step] = logprobs\n",
        "                rb_values[step] = values.flatten()\n",
        "\n",
        "                # compute episodic return\n",
        "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
        "\n",
        "                # if we reach termination or truncation, end\n",
        "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
        "                    end_step = step\n",
        "                    break\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
        "            for t in reversed(range(end_step)):\n",
        "                delta = (\n",
        "                    rb_rewards[t]\n",
        "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
        "                    - rb_values[t]\n",
        "                )\n",
        "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
        "            rb_returns = rb_advantages + rb_values\n",
        "\n",
        "        # convert our episodes to batch of individual transitions\n",
        "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
        "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
        "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
        "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
        "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
        "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_index = np.arange(len(b_obs))\n",
        "        clip_fracs = []\n",
        "        for repeat in range(3):\n",
        "            # shuffle the indices we use to access the data\n",
        "            np.random.shuffle(b_index)\n",
        "            for start in range(0, len(b_obs), batch_size):\n",
        "                # select the indices we want to train on\n",
        "                end = start + batch_size\n",
        "                batch_index = b_index[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
        "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
        "                )\n",
        "                logratio = newlogprob - b_logprobs[batch_index]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clip_fracs += [\n",
        "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
        "                    ]\n",
        "\n",
        "                # normalize advantaegs\n",
        "                advantages = b_advantages[batch_index]\n",
        "                advantages = (advantages - advantages.mean()) / (\n",
        "                    advantages.std() + 1e-8\n",
        "                )\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
        "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
        "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value = value.flatten()\n",
        "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
        "                v_clipped = b_values[batch_index] + torch.clamp(\n",
        "                    value - b_values[batch_index],\n",
        "                    -clip_coef,\n",
        "                    clip_coef,\n",
        "                )\n",
        "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
        "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                v_loss = 0.5 * v_loss_max.mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        print(f\"Training episode {episode}\")\n",
        "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
        "        print(f\"Episode Length: {end_step}\")\n",
        "        print(\"\")\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        print(f\"Value Loss: {v_loss.item()}\")\n",
        "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
        "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
        "        print(f\"Approx KL: {approx_kl.item()}\")\n",
        "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
        "        print(f\"Explained Variance: {explained_var.item()}\")\n",
        "        print(\"\\n-------------------------------------------\\n\")\n",
        "\n",
        "    \"\"\" RENDER THE POLICY \"\"\"\n",
        "    # env = cooperative_pong_v5.parallel_env(render_mode=\"human\")\n",
        "    env = pursuit_v4.parallel_env(render_mode=\"human\", max_cycles=max_cycles)\n",
        "    env = color_reduction_v0(env)\n",
        "    env = resize_v1(env, 64, 64)\n",
        "    env = frame_stack_v1(env, stack_size=4)\n",
        "\n",
        "    agent.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # render 5 episodes out\n",
        "        for episode in range(5):\n",
        "            obs, infos = env.reset(seed=None)\n",
        "            obs = batchify_obs(obs, device)\n",
        "            terms = [False]\n",
        "            truncs = [False]\n",
        "            while not any(terms) and not any(truncs):\n",
        "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
        "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
        "                obs = batchify_obs(obs, device)\n",
        "                terms = [terms[a] for a in terms]\n",
        "                truncs = [truncs[a] for a in truncs]"
      ],
      "metadata": {
        "id": "Qy9D3iuQi9rl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}